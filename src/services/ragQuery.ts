import client, { modelMapping } from '@/lib/azureOpenAI.server';
import { logUsage } from './usage';
import { encode } from 'gpt-tokenizer';

/**
 * RAG/LLM-Query node - Generate answers based on retrieved chunks
 * 
 * @param chunks Array of context chunks for retrieval augmentation
 * @param question The user question to answer
 * @returns Generated answer based on the retrieved context
 * @throws Error if the RAG query fails
 */
export async function runRAGQuery(chunks: string[], question: string): Promise<string> {
  // Input validation
  if (!chunks || !Array.isArray(chunks) || chunks.length === 0) {
    throw new Error('RAG query failed: No context chunks provided');
  }

  if (!question || question.trim() === '') {
    throw new Error('RAG query failed: No question provided');
  }

  try {
    // Build messages array with system prompt and context chunks
    const messages = [
      {
        role: 'system' as const,
        content: 'You are a retrieval-augmented generation assistant.'
      },
      // Include each chunk as a separate user message for context
      ...chunks.map(c => ({
        role: 'user' as const, 
        content: c
      })),
      // Add the actual user question as the final message
      {
        role: 'user' as const,
        content: question
      }
    ];

    // Choose deployment based on token count
    // Use the tokenizer to get an accurate count
    const totalTokens = messages.reduce((acc, msg) => {
      return acc + encode(msg.content).length;
    }, 0);
    
    // Select deployment based on token count
    const deploymentName = totalTokens > 16000 ? modelMapping.omni : modelMapping.turbo;
    console.log(`Using ${deploymentName} model for RAG query (${totalTokens} tokens)`);

    // Ensure client is initialized
    if (!client) {
      throw new Error('Azure OpenAI client is not initialized');
    }
    
    // Call Azure OpenAI chat completions API
    // For Azure OpenAI, we need to use the deploymentId format
    const response = await client.chat.completions.create({
      model: deploymentName,
      messages,
      temperature: 0.3,
      max_tokens: 1000
    }, {
      path: `/openai/deployments/${deploymentName}/chat/completions`
    });

    // Validate response
    if (!response.choices || response.choices.length === 0) {
      throw new Error('No answer generated by Azure OpenAI API');
    }

    const answer = response.choices[0]?.message?.content;
    
    if (!answer) {
      throw new Error('Empty answer returned from Azure OpenAI API');
    }

    // Log usage with system user ID
    await logUsage('rag', {
      promptTokens: response.usage?.prompt_tokens || 0,
      completionTokens: response.usage?.completion_tokens || 0
    }, 'system');

    return answer;
  } catch (error) {
    // Handle specific Azure OpenAI API errors
    if (error instanceof Error) {
      // Specific error handling based on error types
      if (error.message.includes('API key')) {
        throw new Error('Azure OpenAI authentication failed - check API key');
      } else if (error.message.includes('rate limit')) {
        throw new Error('Azure OpenAI rate limit exceeded - try again later');
      } else if (error.message.includes('timeout')) {
        throw new Error('Azure OpenAI request timed out - try again later');
      } else if (error.message.includes('token')) {
        throw new Error('Azure OpenAI token limit exceeded - try with fewer context chunks');
      }
      
      throw new Error(`RAG query failed: ${error.message}`);
    }
    
    throw new Error('RAG query failed due to an unknown error');
  }
}
