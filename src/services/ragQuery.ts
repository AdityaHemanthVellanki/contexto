import { client, modelMapping } from '@/lib/azureOpenAI';
import { logUsage } from './usage';

/**
 * Runs a Retrieval-Augmented Generation (RAG) query using Azure OpenAI
 * 
 * @param chunks Array of context chunks for retrieval augmentation
 * @param question The user question to answer
 * @param userId Authenticated user ID for usage tracking
 * @returns Generated answer based on the retrieved context
 * @throws Error if the RAG query fails or if userId is not provided
 */
export async function runRAGQuery(chunks: string[], question: string, userId: string): Promise<string> {
  // Input validation
  if (!chunks || !Array.isArray(chunks) || chunks.length === 0) {
    throw new Error('RAG query requires context chunks');
  }

  if (!question || question.trim() === '') {
    throw new Error('RAG query requires a valid question');
  }

  if (!userId) {
    throw new Error('User ID is required for RAG query');
  }

  try {
    // Build messages array with system prompt and context chunks
    const messages = [
      {
        role: 'system' as const,
        content: 'You are a retrieval-augmented generation assistant. Answer questions based on the provided context.'
      },
      // Include each chunk as a separate user message for context
      ...chunks.map(chunk => ({
        role: 'user' as const,
        content: `Context: ${chunk}`
      })),
      // Add the actual user question
      {
        role: 'user' as const,
        content: `Question: ${question}\n\nAnswer the question based ONLY on the provided context. If the context doesn't contain the information needed, say so clearly.`
      }
    ];

    // Check total token count - if too large, use the omni model
    // This is a simplistic check; in production you would use a tokenizer
    const totalText = messages.reduce((acc, msg) => acc + msg.content.length, 0);
    const selectedModel = totalText > 12000 ? modelMapping.omni : modelMapping.turbo;

    // Call Azure OpenAI chat completions API
    const response = await client.chat.completions.create({
      model: selectedModel as string,
      messages,
      temperature: 0.3,
      max_tokens: 1000
    });

    // Validate response
    if (!response.choices || response.choices.length === 0) {
      throw new Error('No answer generated by Azure OpenAI API');
    }

    const answer = response.choices[0]?.message?.content;
    
    if (!answer) {
      throw new Error('Empty answer returned from Azure OpenAI API');
    }

    // Log usage with user ID
    await logUsage('ragQuery', {
      promptTokens: response.usage?.prompt_tokens || 0,
      completionTokens: response.usage?.completion_tokens || 0
    }, userId);

    return answer;
  } catch (error) {
    // Handle specific Azure OpenAI API errors
    if (error instanceof Error) {
      // Specific error handling based on error types
      if (error.message.includes('API key')) {
        throw new Error('Azure OpenAI authentication failed - check API key');
      } else if (error.message.includes('rate limit')) {
        throw new Error('Azure OpenAI rate limit exceeded - try again later');
      } else if (error.message.includes('timeout')) {
        throw new Error('Azure OpenAI request timed out - try again later');
      } else if (error.message.includes('token')) {
        throw new Error('Azure OpenAI token limit exceeded - try with fewer context chunks');
      }
      
      throw new Error(`RAG query failed: ${error.message}`);
    }
    
    throw new Error('RAG query failed due to an unknown error');
  }
}
