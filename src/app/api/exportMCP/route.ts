import { NextRequest, NextResponse } from 'next/server';
import { getAuth } from 'firebase-admin/auth';
import { getFirestore } from 'firebase-admin/firestore';
import { getStorage } from 'firebase-admin/storage';
import { app } from '@/lib/firebase';
import JSZip from 'jszip';

// Initialize Firebase Admin services
const auth = getAuth(app);
const db = getFirestore(app);
const storage = getStorage(app);
const bucket = storage.bucket();

export async function POST(request: NextRequest) {
  try {
    // Verify authentication token
    const authHeader = request.headers.get('authorization');
    if (!authHeader || !authHeader.startsWith('Bearer ')) {
      return NextResponse.json({ message: 'Unauthorized: Missing or invalid token' }, { status: 401 });
    }

    const token = authHeader.split('Bearer ')[1];
    const decodedToken = await auth.verifyIdToken(token);
    const userId = decodedToken.uid;

    if (!userId) {
      return NextResponse.json({ message: 'Unauthorized: Invalid user' }, { status: 401 });
    }

    // Get fileId from request body
    const body = await request.json();
    const { fileId } = body;
    
    if (!fileId) {
      return NextResponse.json({ message: 'Bad request: No fileId provided' }, { status: 400 });
    }

    // Verify file ownership
    const uploadRef = db.collection('uploads').doc(fileId);
    const uploadDoc = await uploadRef.get();
    
    if (!uploadDoc.exists) {
      return NextResponse.json({ message: 'File not found' }, { status: 404 });
    }
    
    const uploadData = uploadDoc.data();
    if (uploadData?.userId !== userId) {
      return NextResponse.json({ message: 'Unauthorized: You do not own this file' }, { status: 403 });
    }

    // Get pipeline configuration
    const pipelineRef = db.collection('pipelines').doc(fileId);
    const pipelineDoc = await pipelineRef.get();
    
    if (!pipelineDoc.exists) {
      return NextResponse.json({ message: 'Pipeline not found' }, { status: 404 });
    }
    
    const pipelineData = pipelineDoc.data();
    
    // Create MCP server package
    const zip = new JSZip();
    
    // Create package.json
    const packageJson = {
      name: `contexto-mcp-${fileId}`,
      version: '1.0.0',
      description: 'MCP Server exported from Contexto',
      main: 'index.js',
      scripts: {
        start: 'node index.js'
      },
      dependencies: {
        express: '^4.18.2',
        cors: '^2.8.5',
        'body-parser': '^1.20.2',
        '@azure/openai': '^1.0.0-beta.5',
        dotenv: '^16.0.3'
      }
    };
    
    zip.file('package.json', JSON.stringify(packageJson, null, 2));
    
    // Create .env template
    const envTemplate = `# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_EMBEDDING_DEPLOYMENT=your_embedding_deployment_name
AZURE_COMPLETION_DEPLOYMENT=your_completion_deployment_name

# Server Configuration
PORT=3000
`;
    
    zip.file('.env.template', envTemplate);
    
    // Create README.md
    const readmeContent = `# Contexto MCP Server

This is an exportable MCP (Model Context Protocol) server generated by Contexto.

## Setup Instructions

1. Copy \`.env.template\` to \`.env\` and fill in your API keys
2. Install dependencies:
   \`\`\`
   npm install
   \`\`\`
3. Start the server:
   \`\`\`
   npm start
   \`\`\`

## API Endpoints

- \`POST /query\`: Send queries to the server
  - Body: \`{ "prompt": "Your question here" }\`
  - Returns: \`{ "answer": "AI response" }\`

## Pipeline Configuration

The pipeline is configured for RAG (Retrieval Augmented Generation) with the following components:
- Data source: ${uploadData?.fileName || 'Your uploaded document'}
- Chunker: ${pipelineData?.nodes?.find((n: any) => n.type === 'chunker')?.data?.chunkSize || 1000} token chunks with ${pipelineData?.nodes?.find((n: any) => n.type === 'chunker')?.data?.chunkOverlap || 200} token overlap
- Embeddings: Using Azure OpenAI embedding model
- Retriever: Top-${pipelineData?.nodes?.find((n: any) => n.type === 'retriever')?.data?.topK || 5} retrieval

## Integration

To integrate this MCP server with other tools, use the following URL:
\`http://localhost:3000\`

`;
    
    zip.file('README.md', readmeContent);
    
    // Create index.js
    const serverCode = `require('dotenv').config();
const express = require('express');
const cors = require('cors');
const bodyParser = require('body-parser');
const { OpenAIClient, AzureKeyCredential } = require('@azure/openai');

// Configuration
const PORT = process.env.PORT || 3000;
const AZURE_API_KEY = process.env.AZURE_OPENAI_API_KEY;
const AZURE_ENDPOINT = process.env.AZURE_OPENAI_ENDPOINT;
const COMPLETION_DEPLOYMENT = process.env.AZURE_COMPLETION_DEPLOYMENT;

// Initialize Azure OpenAI Client
const openaiClient = new OpenAIClient(AZURE_ENDPOINT, new AzureKeyCredential(AZURE_API_KEY));

// Initialize Express
const app = express();
app.use(cors());
app.use(bodyParser.json());

// Pipeline configuration
const pipelineConfig = ${JSON.stringify(pipelineData, null, 2)};

// Embedded knowledge base
const knowledgeBase = ${JSON.stringify({
  fileId: fileId,
  fileName: uploadData?.fileName,
  fileType: uploadData?.fileType,
  // In a full implementation, we would include the actual chunks and embeddings here
  // This is a simplified version for the exported MCP server
})};

// Simulated vector database (in production, use a real vector DB)
// These would be the actual chunks and embeddings from your document
const vectorData = [
  {
    id: 1,
    text: "This is a sample chunk of text from your document.",
    // Actual embedding would go here
  },
  // More chunks would be here
];

// API routes
app.post('/query', async (req, res) => {
  try {
    const { prompt } = req.body;
    
    if (!prompt) {
      return res.status(400).json({ message: 'No prompt provided' });
    }
    
    // In a real implementation, this would search the vector database
    // For this example, we'll use simulated chunks
    const context = "This is simulated context from your document that would normally be retrieved using vector similarity search.";
    
    // Create system prompt with context
    const systemPrompt = \`You are an AI assistant helping with document question answering. 
    Answer the user's query based ONLY on the following context. 
    If you cannot find the answer in the context, say that you don't know rather than making up information.
    
    Context:
    \${context}\`;
    
    // Generate completion
    const messages = [
      { role: "system", content: systemPrompt },
      { role: "user", content: prompt }
    ];

    const startTime = Date.now();
    const completionResponse = await openaiClient.getChatCompletions(COMPLETION_DEPLOYMENT, messages, {
      temperature: 0.7,
      maxTokens: 800,
    });
    const endTime = Date.now();
    
    // Get response
    const answer = completionResponse.choices[0].message?.content || "I couldn't generate a response.";
    
    // Return the response
    res.json({
      answer,
      usageReport: {
        promptTokens: completionResponse.usage?.promptTokens || 0,
        completionTokens: completionResponse.usage?.completionTokens || 0,
        totalTokens: completionResponse.usage?.totalTokens || 0,
        latencyMs: endTime - startTime
      }
    });
  } catch (error) {
    console.error('Error processing query:', error);
    res.status(500).json({ 
      message: 'Error processing query',
      error: error.message 
    });
  }
});

// MCP server info endpoint
app.get('/', (req, res) => {
  res.json({
    name: 'Contexto MCP Server',
    version: '1.0.0',
    description: 'MCP Server exported from Contexto',
    endpoints: [
      {
        path: '/query',
        method: 'POST',
        description: 'Query the RAG pipeline',
        parameters: ['prompt']
      }
    ]
  });
});

// Start the server
app.listen(PORT, () => {
  console.log(\`MCP Server running at http://localhost:\${PORT}\`);
  console.log('Ready to accept queries!');
});
`;
    
    zip.file('index.js', serverCode);
    
    // Generate ZIP file
    const zipBlob = await zip.generateAsync({ type: 'blob' });
    const buffer = await zipBlob.arrayBuffer();
    
    // Create a unique identifier for the export
    const timestamp = Date.now();
    const exportId = `${userId}_${timestamp}`;
    const exportFileName = `contexto-mcp-${fileId}-${timestamp}.zip`;
    const exportPath = `users/${userId}/exports/${exportFileName}`;
    
    // Upload to Firebase Storage
    const fileRef = bucket.file(exportPath);
    await fileRef.save(Buffer.from(buffer), {
      contentType: 'application/zip',
      metadata: {
        customMetadata: {
          userId,
          fileId,
          exportedAt: timestamp.toString(),
        },
      },
    });
    
    // Get download URL
    const [exportUrl] = await fileRef.getSignedUrl({
      action: 'read',
      expires: '01-01-2100', // Long expiration
    });
    
    // Store metadata in Firestore exports collection
    const exportRef = db.collection('exports').doc(exportId);
    await exportRef.set({
      userId,
      exportId,
      fileId,
      pipelineId: fileId, // Using fileId as pipelineId for now
      fileName: `${uploadData?.fileName || 'Document'} MCP Server`,
      exportUrl,
      exportPath,
      exportedAt: new Date(), // Server timestamp
      exportType: 'mcp',
    });
    
    // Return ZIP file as response with exportId for tracking
    return new Response(buffer, {
      headers: {
        'Content-Type': 'application/zip',
        'Content-Disposition': `attachment; filename=contexto-mcp-${fileId}.zip`,
        'X-Export-Id': exportId
      }
    });

  } catch (error) {
    console.error('MCP export error:', error);
    const message = error instanceof Error ? error.message : 'Unknown error';
    return NextResponse.json({ message: `MCP export failed: ${message}` }, { status: 500 });
  }
}
