import { NextRequest, NextResponse } from 'next/server';
import { initializeFirebaseAdmin } from '@/lib/firebase-admin-init';
import { getFirebaseAuth } from '@/lib/firebase-admin-init';
import { authenticateRequest } from '@/lib/api-auth';
import { rateLimit } from '@/lib/rate-limiter-memory';
import { r2, R2_BUCKET } from '@/lib/r2';
import { PutObjectCommand } from '@aws-sdk/client-s3';
import { FieldValue } from 'firebase-admin/firestore';
import JSZip from 'jszip';

// Initialize Firebase Admin SDK at module load time
// This ensures Firebase is ready before any requests are processed
try {
  // This will initialize Firebase Admin if not already initialized
  initializeFirebaseAdmin();
  console.log('✅ Firebase initialized successfully for exportMCP API');
} catch (error) {
  console.error('❌ Firebase initialization failed in exportMCP API:', 
    error instanceof Error ? error.message : String(error));
  // The error will be handled when the API route is called - no fallbacks
}

export async function POST(request: NextRequest) {
  try {
    // Apply rate limiting - 5 requests per 30 seconds per user/IP
    const rateLimitResult = await rateLimit(request, {
      limit: 5,
      windowSizeInSeconds: 30
    });
    
    // Return rate limit response if limit exceeded
    if (rateLimitResult.limited) {
      return rateLimitResult.response || NextResponse.json(
        { message: 'Rate limit exceeded', error: 'rate_limited' },
        { status: 429 }
      );
    }
    
    // Authenticate request
    const auth = await authenticateRequest(request);
    
    if (!auth.authenticated) {
      return auth.response;
    }
    
    const userId = auth.userId;

    if (!userId) {
      return NextResponse.json({ message: 'Unauthorized: Invalid user' }, { status: 401 });
    }

    // Get fileId from request body
    const body = await request.json();
    const { fileId } = body;
    
    if (!fileId) {
      return NextResponse.json({ message: 'Bad request: No fileId provided' }, { status: 400 });
    }

    // Verify file ownership
    // Get Firestore instance using our improved initialization approach
    const db = initializeFirebaseAdmin();
    const uploadRef = db.collection('uploads').doc(fileId);
    const uploadDoc = await uploadRef.get();
    
    if (!uploadDoc.exists) {
      return NextResponse.json({ message: 'File not found' }, { status: 404 });
    }
    
    const uploadData = uploadDoc.data();
    if (uploadData?.userId !== userId) {
      return NextResponse.json({ message: 'Unauthorized: You do not own this file' }, { status: 403 });
    }

    // Get pipeline configuration
    // Using the db instance we already initialized above
    const pipelineRef = db.collection('pipelines').doc(fileId);
    const pipelineDoc = await pipelineRef.get();
    
    if (!pipelineDoc.exists) {
      return NextResponse.json({ message: 'Pipeline not found' }, { status: 404 });
    }
    
    const pipelineData = pipelineDoc.data();
    
    // Create MCP server package
    const zip = new JSZip();
    
    // Create package.json
    const packageJson = {
      name: `contexto-mcp-${fileId}`,
      version: '1.0.0',
      description: 'MCP Server exported from Contexto',
      main: 'index.js',
      scripts: {
        start: 'node index.js'
      },
      dependencies: {
        express: '^4.18.2',
        cors: '^2.8.5',
        'body-parser': '^1.20.2',
        '@azure/openai': '^1.0.0-beta.5',
        dotenv: '^16.0.3'
      }
    };
    
    zip.file('package.json', JSON.stringify(packageJson, null, 2));
    
    // Create .env template
    const envTemplate = `# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_EMBEDDING_DEPLOYMENT=your_embedding_deployment_name
AZURE_COMPLETION_DEPLOYMENT=your_completion_deployment_name

# Server Configuration
PORT=3000
`;
    
    zip.file('.env.template', envTemplate);
    
    // Create README.md
    const readmeContent = `# Contexto MCP Server

This is an exportable MCP (Model Context Protocol) server generated by Contexto.

## Setup Instructions

1. Copy \`.env.template\` to \`.env\` and fill in your API keys
2. Install dependencies:
   \`\`\`
   npm install
   \`\`\`
3. Start the server:
   \`\`\`
   npm start
   \`\`\`

## API Endpoints

- \`POST /query\`: Send queries to the server
  - Body: \`{ "prompt": "Your question here" }\`
  - Returns: \`{ "answer": "AI response" }\`

## Pipeline Configuration

The pipeline is configured for RAG (Retrieval Augmented Generation) with the following components:
- Data source: ${uploadData?.fileName || 'Your uploaded document'}
- Chunker: ${pipelineData?.nodes?.find((n: any) => n.type === 'chunker')?.data?.chunkSize || 1000} token chunks with ${pipelineData?.nodes?.find((n: any) => n.type === 'chunker')?.data?.chunkOverlap || 200} token overlap
- Embeddings: Using Azure OpenAI embedding model
- Retriever: Top-${pipelineData?.nodes?.find((n: any) => n.type === 'retriever')?.data?.topK || 5} retrieval

## Integration

To integrate this MCP server with other tools, use the following URL:
\`http://localhost:3000\`

`;
    
    zip.file('README.md', readmeContent);
    
    // Create index.js
    const serverCode = `require('dotenv').config();
const express = require('express');
const cors = require('cors');
const bodyParser = require('body-parser');
const { OpenAIClient, AzureKeyCredential } = require('@azure/openai');

// Configuration
const PORT = process.env.PORT || 3000;
const AZURE_API_KEY = process.env.AZURE_OPENAI_API_KEY;
const AZURE_ENDPOINT = process.env.AZURE_OPENAI_ENDPOINT;
const COMPLETION_DEPLOYMENT = process.env.AZURE_COMPLETION_DEPLOYMENT;

// Initialize Azure OpenAI Client
const openaiClient = new OpenAIClient(AZURE_ENDPOINT, new AzureKeyCredential(AZURE_API_KEY));

// Initialize Express
const app = express();
app.use(cors());
app.use(bodyParser.json());

// Pipeline configuration
const pipelineConfig = ${JSON.stringify(pipelineData, null, 2)};

// Embedded knowledge base
const knowledgeBase = ${JSON.stringify({
  fileId: fileId,
  fileName: uploadData?.fileName,
  fileType: uploadData?.fileType,
  // In a full implementation, we would include the actual chunks and embeddings here
  // This is a simplified version for the exported MCP server
})};

// Simulated vector database (in production, use a real vector DB)
// These would be the actual chunks and embeddings from your document
const vectorData = [
  {
    id: 1,
    text: "This is a sample chunk of text from your document.",
    // Actual embedding would go here
  },
  // More chunks would be here
];

// API routes
app.post('/query', async (req, res) => {
  try {
    const { prompt } = req.body;
    
    if (!prompt) {
      return res.status(400).json({ message: 'No prompt provided' });
    }
    
    // In a real implementation, this would search the vector database
    // For this example, we'll use simulated chunks
    const context = "This is simulated context from your document that would normally be retrieved using vector similarity search.";
    
    // Create system prompt with context
    const systemPrompt = \`You are an AI assistant helping with document question answering. 
    Answer the user's query based ONLY on the following context. 
    If you cannot find the answer in the context, say that you don't know rather than making up information.
    
    Context:
    \${context}\`;
    
    // Generate completion
    const messages = [
      { role: "system", content: systemPrompt },
      { role: "user", content: prompt }
    ];

    const startTime = Date.now();
    const completionResponse = await openaiClient.getChatCompletions(COMPLETION_DEPLOYMENT, messages, {
      temperature: 0.7,
      maxTokens: 800,
    });
    const endTime = Date.now();
    
    // Get response
    const answer = completionResponse.choices[0].message?.content || "I couldn't generate a response.";
    
    // Return the response
    res.json({
      answer,
      usageReport: {
        promptTokens: completionResponse.usage?.promptTokens || 0,
        completionTokens: completionResponse.usage?.completionTokens || 0,
        totalTokens: completionResponse.usage?.totalTokens || 0,
        latencyMs: endTime - startTime
      }
    });
  } catch (error) {
    console.error('Error processing query:', error);
    res.status(500).json({ 
      message: 'Error processing query',
      error: error.message 
    });
  }
});

// MCP server info endpoint
app.get('/', (req, res) => {
  res.json({
    name: 'Contexto MCP Server',
    version: '1.0.0',
    description: 'MCP Server exported from Contexto',
    endpoints: [
      {
        path: '/query',
        method: 'POST',
        description: 'Query the RAG pipeline',
        parameters: ['prompt']
      }
    ]
  });
});

// Start the server
app.listen(PORT, () => {
  console.log(\`MCP Server running at http://localhost:\${PORT}\`);
  console.log('Ready to accept queries!');
});
`;
    
    zip.file('index.js', serverCode);
    
    // Generate ZIP file
    const zipBlob = await zip.generateAsync({ type: 'blob' });
    const buffer = await zipBlob.arrayBuffer();
    
    // Create a unique identifier for the export
    const timestamp = Date.now();
    const exportId = `${userId}_${timestamp}`;
    const exportFileName = `contexto-mcp-${fileId}-${timestamp}.zip`;
    const exportPath = `users/${userId}/exports/${exportFileName}`;
    
    // Upload to Cloudflare R2 instead of Firebase Storage
    const r2Key = `${userId}/exports/${exportFileName}`;
    
    try {
      await r2.send(
        new PutObjectCommand({
          Bucket: R2_BUCKET,
          Key: r2Key,
          Body: Buffer.from(buffer),
          ContentType: 'application/zip'
        })
      );
      
      console.log(`MCP export successfully uploaded to R2: ${r2Key}`);
    } catch (r2Error) {
      console.error('Error uploading to R2:', r2Error);
      throw new Error(`Failed to upload export to R2: ${r2Error instanceof Error ? r2Error.message : 'Unknown error'}`);
    }
    
    // Generate R2 file URL
    const exportUrl = process.env.CF_R2_ENDPOINT 
      ? `${process.env.CF_R2_ENDPOINT}/${R2_BUCKET}/${encodeURIComponent(r2Key)}`
      : `https://${R2_BUCKET}.r2.cloudflarestorage.com/${encodeURIComponent(r2Key)}`;
    
    // Store metadata in Firestore exports collection
    // Using the db instance we already initialized above
    const exportRef = db.collection('exports').doc(exportId);
    await exportRef.set({
      userId,
      exportId,
      fileId,
      pipelineId: fileId, // Using fileId as pipelineId for now
      fileName: `${uploadData?.fileName || 'Document'} MCP Server`,
      exportUrl,
      r2Key, // Store R2 key instead of Firebase path
      fileSize: buffer.byteLength,
      contentType: 'application/zip',
      exportedAt: FieldValue.serverTimestamp(),
      exportType: 'mcp',
    });
    
    // Return ZIP file as response with exportId for tracking
    return new Response(buffer, {
      headers: {
        'Content-Type': 'application/zip',
        'Content-Disposition': `attachment; filename=contexto-mcp-${fileId}.zip`,
        'X-Export-Id': exportId
      }
    });

  } catch (error) {
    console.error('MCP export error:', error);
    const message = error instanceof Error ? error.message : 'Unknown error';
    return NextResponse.json({ message: `MCP export failed: ${message}` }, { status: 500 });
  }
}
