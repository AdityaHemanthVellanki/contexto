import { z } from 'zod';
import { getR2Client, R2_BUCKET } from './r2';
import { PutObjectCommand } from '@aws-sdk/client-s3';
import JSZip from 'jszip';

// Pipeline schema for validation
const PipelineSchema = z.object({
  id: z.string(),
  metadata: z.object({
    author: z.string(),
    createdAt: z.string(),
    fileName: z.string(),
    fileType: z.string(),
    purpose: z.string(),
    vectorStore: z.string(),
    chunksCount: z.number(),
    chunkSize: z.number(),
    overlap: z.number()
  }),
  nodes: z.array(z.object({
    id: z.string(),
    type: z.string(),
    data: z.record(z.any())
  })),
  edges: z.array(z.object({
    id: z.string(),
    source: z.string(),
    target: z.string()
  }))
});

type Pipeline = z.infer<typeof PipelineSchema>;

type ServerEnv = {
  env: {
    NODE_ENV: 'development' | 'production';
    cwd?: string;
  };
};

/**
 * Export MCP pipeline as a downloadable ZIP package
 */
export async function exportMCPPipeline(pipeline: Pipeline, userId: string): Promise<string> {
  try {
    // Validate pipeline structure
    const validatedPipeline = PipelineSchema.parse(pipeline);
    
    console.log(`Exporting MCP pipeline: ${validatedPipeline.id}`);

    // Create ZIP package
    const zip = new JSZip();

    // 1. Add pipeline.json
    zip.file('pipeline.json', JSON.stringify(validatedPipeline, null, 2));

    // 2. Add MCP server implementation
    const serverJs = generateMCPServer(validatedPipeline);
    zip.file('server.js', serverJs);

    // 3. Add package.json
    const packageJson = generatePackageJson(validatedPipeline);
    zip.file('package.json', JSON.stringify(packageJson, null, 2));

    // 4. Add vectorStoreClient.js
    const fs = require('fs');
    const path = require('path');
    const vectorStoreClientPath = path.join(__dirname, 'templates', 'vectorStoreClient.js');
    const vectorStoreClient = fs.readFileSync(vectorStoreClientPath, 'utf8');
    zip.file('vectorStoreClient.js', vectorStoreClient);

    // 5. Add Dockerfile
    const dockerfile = generateDockerfile();
    zip.file('Dockerfile', dockerfile);

    // 5. Add README.md
    const readme = generateReadme(validatedPipeline);
    zip.file('README.md', readme);

    // 6. Add .env.example
    const envExample = generateEnvExample(validatedPipeline);
    zip.file('.env.example', envExample);

    // Generate ZIP buffer
    const zipBuffer = await zip.generateAsync({ type: 'nodebuffer' });

    // Upload to R2
    const exportId = `${userId}_${Date.now()}`;
    const r2Key = `users/${userId}/exports/${exportId}/mcp-pipeline.zip`;

    const client = getR2Client();
    if (!client) {
      throw new Error('R2 client not initialized');
    }
    
    const command = new PutObjectCommand({
      Bucket: R2_BUCKET,
      Key: r2Key,
      Body: zipBuffer,
      ContentType: 'application/zip',
      ContentDisposition: `attachment; filename="mcp-pipeline-${validatedPipeline.id}.zip"`
    });
    
    const result = await client.send(command);

    // Use the export service to generate a presigned URL
    const { getPipelineExportUrl } = await import('../services/exportService');
    const downloadUrl = await getPipelineExportUrl(userId, exportId);
    
    // Log the generated URL (without query params for security)
    const urlObj = new URL(downloadUrl);
    console.log('mcpExporter: generated presigned URL for', `${urlObj.origin}${urlObj.pathname}`);

    // Log export to Firestore
    await logExport(exportId, validatedPipeline.id, userId, r2Key, downloadUrl);

    console.log(`MCP pipeline exported successfully: ${downloadUrl}`);
    return downloadUrl;

  } catch (error) {
    console.error('MCP export error:', error);
    throw new Error(`Failed to export MCP pipeline: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Generate MCP server implementation
 */
function generateMCPServer(pipeline: Pipeline): string {
  return `#!/usr/bin/env node

/**
 * MCP Server for Pipeline: ${pipeline.id}
 * Generated by Contexto
 * 
 * This server implements the Model Context Protocol (MCP) specification
 * for the pipeline processing workflow defined in pipeline.json
 */

const { Server } = require('@modelcontextprotocol/sdk/server/index.js');
const { StdioServerTransport } = require('@modelcontextprotocol/sdk/server/stdio.js');
const { CallToolRequestSchema, ListToolsRequestSchema } = require('@modelcontextprotocol/sdk/types.js');

// Pipeline metadata
const PIPELINE_METADATA = ${JSON.stringify(pipeline.metadata, null, 2)};

class MCPPipelineServer {
  constructor() {
    this.server = new Server(
      {
        name: 'contexto-mcp-pipeline',
        version: '1.0.0',
      },
      {
        capabilities: {
          tools: {},
        },
      }
    );

    this.setupToolHandlers();
    this.setupErrorHandling();
  }

  setupToolHandlers() {
    // List available tools
    this.server.setRequestHandler(ListToolsRequestSchema, async () => {
      return {
        tools: [
          {
            name: 'process_data',
            description: \`Process data through the ${pipeline.metadata.vectorStore} pipeline\`,
            inputSchema: {
              type: 'object',
              properties: {
                input: {
                  type: 'string',
                  description: 'Input data to process'
                },
                options: {
                  type: 'object',
                  description: 'Processing options',
                  properties: {
                    chunkSize: { type: 'number', default: ${pipeline.metadata.chunkSize} },
                    overlap: { type: 'number', default: ${pipeline.metadata.overlap} },
                    topK: { type: 'number', default: 5 }
                  }
                }
              },
              required: ['input']
            }
          },
          {
            name: 'get_pipeline_info',
            description: 'Get information about this pipeline',
            inputSchema: {
              type: 'object',
              properties: {}
            }
          }
        ]
      };
    });

    // Handle tool calls
    this.server.setRequestHandler(CallToolRequestSchema, async (request) => {
      const { name, arguments: args } = request.params;

      switch (name) {
        case 'process_data':
          return await this.processData(args.input, args.options || {});
        
        case 'get_pipeline_info':
          return this.getPipelineInfo();
        
        default:
          throw new Error(\`Unknown tool: \${name}\`);
      }
    });
  }

  async processData(input, options = {}) {
    try {
      // Process data through the pipeline with real implementation
      const chunks = this.chunkText(input, options.chunkSize || ${pipeline.metadata.chunkSize});
      
      // Initialize vector store client (from environment configuration)
      const vectorStoreClient = require('./vectorStoreClient');
      
      // Process chunks through vector store (real implementation)
      try {
        // Embed text chunks
        const embedResult = await vectorStoreClient.embed(chunks);
        
        // Get embedding model name for reporting
        const embeddingModel = embedResult && embedResult.model ? embedResult.model : 'default';
        
        // Index embeddings in vector database
        await vectorStoreClient.index(embedResult.embeddings, chunks, {
          metadata: {
            source: 'mcp_server_request',
            timestamp: new Date().toISOString()
          }
        });
        
        return {
          content: [
            {
              type: 'text',
              text: \`Successfully processed \${chunks.length} chunks through ${pipeline.metadata.vectorStore} pipeline.\n\nOriginal purpose: ${pipeline.metadata.purpose}\n\nEmbeddings created and indexed with \${embeddingModel} model.\nVector store: ${pipeline.metadata.vectorStore}\nIndex status: Complete\`
            }
          ]
        };
      } catch (vectorError) {
        console.error('Vector processing error:', vectorError);
        throw new Error(\`Vector processing failed: \${vectorError.message}\`);
      }
    } catch (error) {
      throw new Error(\`Processing failed: \${error.message}\`);
    }
  }

  getPipelineInfo() {
    return {
      content: [
        {
          type: 'text',
          text: \`Pipeline Information:
- ID: ${pipeline.id}
- Created: ${pipeline.metadata.createdAt}
- Source File: ${pipeline.metadata.fileName} (${pipeline.metadata.fileType})
- Purpose: ${pipeline.metadata.purpose}
- Vector Store: ${pipeline.metadata.vectorStore}
- Chunks: ${pipeline.metadata.chunksCount}
- Chunk Size: ${pipeline.metadata.chunkSize}
- Overlap: ${pipeline.metadata.overlap}\`
        }
      ]
    };
  }

  chunkText(text, chunkSize) {
    const client = getR2Client();
    if (!client) {
      throw new Error('R2 client not initialized');
    }
    const chunks = [];
    const overlap = Math.floor(chunkSize * 0.1);
    
    for (let i = 0; i < text.length; i += chunkSize - overlap) {
      const chunk = text.slice(i, i + chunkSize);
      if (chunk.trim()) {
        chunks.push(chunk.trim());
      }
    }
    
    return chunks;
  }

  setupErrorHandling() {
    this.server.onerror = (error) => {
      console.error('[MCP Error]', error);
    };

    process.on('SIGINT', async () => {
      await this.server.close();
      process.exit(0);
    });
  }

  async run() {
    const transport = new StdioServerTransport();
    await this.server.connect(transport);
    console.error('MCP Pipeline Server running on stdio');
  }
}

// Start the server
if (require.main === module) {
  const server = new MCPPipelineServer();
  server.run().catch(console.error);
}

module.exports = MCPPipelineServer;
`;

}

/**
 * Generate package.json for the MCP server
 */
function generatePackageJson(pipeline: Pipeline): object {
  return {
    name: `mcp-pipeline-${pipeline.id}`,
    version: '1.0.0',
    description: `MCP Server for pipeline: ${pipeline.metadata.purpose}`,
    main: 'server.js',
    scripts: {
      start: 'node server.js',
      dev: 'node server.js'
    },
    dependencies: {
      '@modelcontextprotocol/sdk': '^0.4.0'
    },
    engines: {
      node: '>=18.0.0'
    },
    keywords: ['mcp', 'model-context-protocol', 'contexto', 'pipeline'],
    author: 'Contexto MCP Builder',
    license: 'MIT'
  };
}

/**
 * Generate Dockerfile for containerized deployment
 */
function generateDockerfile(): string {
  return `FROM node:18-alpine

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy application files
COPY . .

# Create non-root user
RUN addgroup -g 1001 -S nodejs
RUN adduser -S mcp -u 1001

# Change ownership
RUN chown -R mcp:nodejs /app
USER mcp

# Expose port (if needed for HTTP transport)
EXPOSE 3000

# Start the MCP server
CMD ["npm", "start"]
`;
}

/**
 * Generate README.md documentation
 */
function generateReadme(pipeline: Pipeline): string {
  return `# MCP Pipeline: ${pipeline.id}

Generated by [Contexto](https://contexto.dev) - The AI-powered MCP pipeline builder.

## Overview

This MCP (Model Context Protocol) server implements a data processing pipeline based on your uploaded file and requirements.

**Pipeline Details:**
- **Source File:** ${pipeline.metadata.fileName} (${pipeline.metadata.fileType})
- **Purpose:** ${pipeline.metadata.purpose}
- **Vector Store:** ${pipeline.metadata.vectorStore}
- **Processing:** ${pipeline.metadata.chunksCount} chunks, ${pipeline.metadata.chunkSize} tokens each

## Quick Start

### Prerequisites
- Node.js 18+ installed
- MCP-compatible client (e.g., Claude Desktop)

### Installation

1. Extract this ZIP file
2. Install dependencies:
   \`\`\`bash
   npm install
   \`\`\`

3. Configure environment variables:
   \`\`\`bash
   cp .env.example .env
   # Edit .env with your API keys
   \`\`\`

4. Start the server:
   \`\`\`bash
   npm start
   \`\`\`

### Docker Deployment

\`\`\`bash
docker build -t mcp-pipeline-${pipeline.id} .
docker run -p 3000:3000 mcp-pipeline-${pipeline.id}
\`\`\`

## Available Tools

### \`process_data\`
Process input data through the configured pipeline.

**Parameters:**
- \`input\` (string, required): Data to process
- \`options\` (object, optional): Processing options
  - \`chunkSize\` (number): Override default chunk size (${pipeline.metadata.chunkSize})
  - \`overlap\` (number): Override default overlap (${pipeline.metadata.overlap})
  - \`topK\` (number): Number of results to return (default: 5)

### \`get_pipeline_info\`
Get detailed information about this pipeline configuration.

## Pipeline Architecture

${pipeline.nodes.map(node => `- **${node.type}**: ${node.id}`).join('\n')}

## Configuration

See \`.env.example\` for required environment variables.

## Support

For issues or questions about this generated MCP server:
1. Check the [Contexto documentation](https://docs.contexto.dev)
2. Visit [GitHub Issues](https://github.com/contexto/issues)
3. Contact support@contexto.dev

---

*Generated on ${new Date().toISOString()} by Contexto MCP Builder*
`;
}

/**
 * Generate .env.example file
 */
function generateEnvExample(pipeline: Pipeline): string {
  const envVars = [
    '# MCP Pipeline Environment Configuration',
    `# Generated for pipeline: ${pipeline.id}`,
    '',
    '# Required for vector store operations'
  ];

  // Add environment variables based on vector store type
  switch (pipeline.metadata.vectorStore) {
    case 'pinecone':
      envVars.push(
        'PINECONE_API_KEY=your_pinecone_api_key_here',
        'PINECONE_INDEX_NAME=contexto-index'
      );
      break;
    case 'qdrant':
      envVars.push(
        'QDRANT_URL=https://your-cluster-url.qdrant.tech',
        'QDRANT_API_KEY=your_qdrant_api_key_here'
      );
      break;
    case 'supabase':
      envVars.push(
        'SUPABASE_URL=https://your-project.supabase.co',
        'SUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here'
      );
      break;
    default:
      envVars.push('# No additional configuration required for Firestore');
  }

  envVars.push(
    '',
    '# Optional: Azure OpenAI for embeddings and chat',
    'AZURE_OPENAI_API_KEY=your_azure_openai_key_here',
    'AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com',
    'AZURE_OPENAI_DEPLOYMENT=gpt-4',
    'AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002'
  );

  return envVars.join('\n');
}

/**
 * Log export to Firestore for analytics
 */
async function logExport(
  exportId: string,
  pipelineId: string,
  userId: string,
  r2Key: string,
  downloadUrl: string
): Promise<void> {
  try {
    // Use the firestore-admin module which properly initializes Firebase Admin
    const { getFirestoreAdmin } = await import('./firestore-admin');
    const db = await getFirestoreAdmin();
    
    // Log detailed information for debugging
    console.log(`Attempting to log export with ID: ${exportId}`);
    console.log(`User ID for export: ${userId}`);
    
    // Create export document with complete metadata
    const exportData = {
      exportId,
      pipelineId,
      userId, // Make sure this matches the user's Firebase Auth UID
      r2Key,
      downloadUrl,
      status: 'completed',
      fileType: 'mcp-pipeline',
      createdAt: new Date(), // Use JavaScript Date object for server-side operations
      expiresAt: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000), // 7 days
      metadata: {
        exportType: 'mcp',
        timestamp: new Date().toISOString()
      }
    };
    
    // Log the data being written for debugging
    console.log('Writing export data to Firestore:', JSON.stringify(exportData, null, 2));
    
    // Write to Firestore using admin SDK which bypasses security rules
    await db.collection('exports').doc(exportId).set(exportData);

    console.log(`Successfully logged export: ${exportId} for user ${userId}`);
  } catch (error) {
    // Detailed error logging
    console.error('Failed to log export to Firestore:', error);
    console.error('Error details:', JSON.stringify(error, Object.getOwnPropertyNames(error)));
    // Don't throw error for logging failures to prevent blocking the export process
  }
}
