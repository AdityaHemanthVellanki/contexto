import { generateUploadUrl } from '@/lib/r2-client';
import { getFirestore } from '@/lib/firebase-admin';
import { v4 as uuidv4 } from 'uuid';
import * as fs from 'fs';
import * as path from 'path';
import * as os from 'os';
import archiver from 'archiver';

interface PipelineData {
  id: string;
  userId: string;
  metadata: {
    fileName: string;
    purpose: string;
    vectorStore: string;
    chunksCount: number;
    chunkSize: number;
    overlap: number;
    indexName?: string;
    namespace?: string;
  };
  config: {
    embedding: {
      model: string;
      provider: string;
    };
    indexing: {
      backend: string;
    };
    retrieval: {
      topK: number;
      searchType: string;
    };
  };
}

/**
 * Generate MCP server package.json
 */
function generatePackageJson(pipelineId: string): string {
  return JSON.stringify({
    name: `mcp-pipeline-${pipelineId}`,
    version: '1.0.0',
    description: 'MCP Server generated by Contexto',
    main: 'server.js',
    scripts: {
      start: 'node server.js',
      dev: 'node server.js'
    },
    dependencies: {
      '@modelcontextprotocol/sdk': '^0.4.0',
      'dotenv': '^16.0.0',
      'express': '^4.18.2',
      '@pinecone-database/pinecone': '^4.0.0',
      'openai': '^4.56.0'
    },
    engines: {
      node: '>=18.0.0'
    }
  }, null, 2);
}

/**
 * Generate MCP server.js file
 */
function generateServerJs(pipeline: PipelineData): string {
  const defaultIndexName = (pipeline as any).indexName || pipeline.metadata.indexName || pipeline.metadata.vectorStore || 'ctx-shared';
  const defaultNamespace = (pipeline as any).namespace || pipeline.metadata.namespace || '';
  return `#!/usr/bin/env node

const { Server } = require('@modelcontextprotocol/sdk/server/index.js');
const { StdioServerTransport } = require('@modelcontextprotocol/sdk/server/stdio.js');
const { CallToolRequestSchema, ListToolsRequestSchema } = require('@modelcontextprotocol/sdk/types.js');
const express = require('express');
const { Pinecone } = require('@pinecone-database/pinecone');
const OpenAI = require('openai');

// Load environment variables
require('dotenv').config();

const server = new Server({
  name: 'contexto-pipeline-${pipeline.id}',
  version: '1.0.0',
  description: 'RAG pipeline for ${pipeline.metadata.purpose}'
}, {
  capabilities: {
    tools: {}
  }
});

// Tool definitions
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: 'search_knowledge',
        description: 'Search the knowledge base for relevant information',
        inputSchema: {
          type: 'object',
          properties: {
            query: {
              type: 'string',
              description: 'The search query'
            },
            limit: {
              type: 'number',
              description: 'Maximum number of results to return',
              default: ${pipeline.config.retrieval.topK}
            }
          },
          required: ['query']
        }
      },
      {
        name: 'get_pipeline_info',
        description: 'Get information about this pipeline',
        inputSchema: {
          type: 'object',
          properties: {}
        }
      }
    ]
  };
});

// Tool handlers
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;
  
  switch (name) {
    case 'search_knowledge':
      return await searchKnowledge(args.query, args.limit || ${pipeline.config.retrieval.topK});
    
    case 'get_pipeline_info':
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              pipelineId: '${pipeline.id}',
              purpose: '${pipeline.metadata.purpose}',
              vectorStore: '${pipeline.metadata.vectorStore}',
              chunksCount: ${pipeline.metadata.chunksCount},
              chunkSize: ${pipeline.metadata.chunkSize},
              overlap: ${pipeline.metadata.overlap},
              retrievalConfig: {
                topK: ${pipeline.config.retrieval.topK},
                searchType: '${pipeline.config.retrieval.searchType}'
              }
            }, null, 2)
          }
        ]
      };
    
    default:
      throw new Error(\`Unknown tool: \${name}\`);
  }
});

// Start a minimal HTTP server to satisfy Heroku's web dyno requirements
const app = express();
const PORT = process.env.PORT || 3000;
app.get('/', (_req, res) => res.send('Contexto MCP server is running for pipeline ${pipeline.id}'));
app.get('/health', (_req, res) => res.send('ok'));
app.listen(PORT, () => {
  console.error('HTTP server listening on port ' + PORT);
});

// Search function implementation
async function searchKnowledge(query, limit = ${pipeline.config.retrieval.topK}) {
  try {
    // Validate env
    const AZKEY = process.env.AZURE_OPENAI_API_KEY;
    const AZEP = (process.env.AZURE_OPENAI_ENDPOINT || '').replace(/\/$/, '')
    const AZ_EMBED_DEP = process.env.AZURE_OPENAI_DEPLOYMENT_EMBEDDING;
    const AZ_CHAT_DEP = process.env.AZURE_OPENAI_DEPLOYMENT_GPT4 || process.env.AZURE_OPENAI_DEPLOYMENT_TURBO;
    const AZ_API_VERSION = process.env.AZURE_OPENAI_API_VERSION || '2024-02-15-preview';
    const PINECONE_API_KEY = process.env.PINECONE_API_KEY;
    const DEFAULT_INDEX_NAME = process.env.PINECONE_INDEX || process.env.PINECONE_INDEX_NAME || '${defaultIndexName}';
    const DEFAULT_NAMESPACE = process.env.PINECONE_NAMESPACE || '${defaultNamespace}';

    if (!PINECONE_API_KEY) {
      throw new Error('Missing PINECONE_API_KEY');
    }
    if (!AZKEY || !AZEP || !AZ_EMBED_DEP) {
      throw new Error('Missing Azure OpenAI embedding configuration (AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_EMBEDDING)');
    }

    // Init Pinecone client
    const pc = new Pinecone({ apiKey: PINECONE_API_KEY });
    const index = pc.index(DEFAULT_INDEX_NAME);

    // Init Azure OpenAI clients
    const embedClient = new OpenAI({
      apiKey: AZKEY,
      baseURL: AZEP + '/openai/deployments/' + AZ_EMBED_DEP,
      defaultQuery: { 'api-version': AZ_API_VERSION },
      defaultHeaders: { 'api-key': AZKEY }
    });

    const chatEnabled = Boolean(AZ_CHAT_DEP);
    const chatClient = chatEnabled ? new OpenAI({
      apiKey: AZKEY,
      baseURL: AZEP + '/openai/deployments/' + AZ_CHAT_DEP,
      defaultQuery: { 'api-version': AZ_API_VERSION },
      defaultHeaders: { 'api-key': AZKEY }
    }) : null;

    // Create query embedding
    const emb = await embedClient.embeddings.create({ input: query, model: AZ_EMBED_DEP });
    const vector = emb?.data?.[0]?.embedding;
    if (!vector) throw new Error('Failed to generate query embedding');

    // Query Pinecone (namespace per user)
    const ns = DEFAULT_NAMESPACE || undefined;
    const results = await index.namespace(ns).query({
      topK: Number(limit) || ${pipeline.config.retrieval.topK},
      vector,
      includeMetadata: true
    });

    const matches = results?.matches || [];
    if (!matches.length) {
      return {
        content: [{ type: 'text', text: \`No results found for: "\${query}"\` }]
      };
    }

    // Build context from top matches
    const sources = matches.map((m, i) => ({
      score: m.score,
      text: m?.metadata?.text || '',
      fileName: m?.metadata?.fileName || '${pipeline.metadata.fileName}',
      chunkIndex: m?.metadata?.chunkIndex ?? i
    }));
    const context = sources.map((s, i) => \`[\${i+1}] \${s.text}\`).join('\n\n');

    // Optional summarization via Azure OpenAI
    let answer = \`Top \${Math.min(sources.length, Number(limit) || ${pipeline.config.retrieval.topK})} results for: "\${query}"\n\n\${sources.map((s,i)=>\`[\${i+1}] (\${(s.score||0).toFixed(4)}) \${s.text.substring(0,300)}\${s.text.length>300?'â€¦':''}\`).join('\n\n')}\`;
    if (chatEnabled && chatClient) {
      try {
        const completion = await chatClient.chat.completions.create({
          model: AZ_CHAT_DEP,
          temperature: 0.2,
          messages: [
            { role: 'system', content: 'You are a helpful assistant. Use the provided context excerpts to answer.' },
            { role: 'user', content: \`Context:\n\${context}\n\nQuestion: \${query}\n\nProvide a concise answer and cite sources as [1], [2], etc.\` }
          ]
        });
        const msg = completion?.choices?.[0]?.message?.content;
        if (msg) answer = msg;
      } catch (e) {
        // Fallback to raw snippets if chat fails
      }
    }

    // Return MCP tool response format
    return {
      content: [
        { type: 'text', text: answer },
        { type: 'text', text: \`\n\nSources:\n\${sources.map((s,i)=>\`[\${i+1}] \${s.fileName}#\${s.chunkIndex}\`).join('\n')}\` }
      ]
    };
  } catch (error) {
    return {
      content: [
        {
          type: 'text',
          text: \`Error searching knowledge base: \${error.message}\`
        }
      ]
    };
  }
}

// Start the server
async function main() {
  const transport = new StdioServerTransport();
  await server.connect(transport);
  console.error('MCP Server started for pipeline ${pipeline.id}');
}

if (require.main === module) {
  main().catch(console.error);
}
`;
}

/**
 * Generate .env.example file
 */
function generateEnvExample(): string {
  return `# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_EMBEDDING=text-embedding-ada-002
AZURE_OPENAI_DEPLOYMENT_TURBO=gpt-35-turbo
AZURE_OPENAI_DEPLOYMENT_GPT4=gpt-4
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX_NAME=contexto-shared
# or
PINECONE_INDEX=contexto-shared
PINECONE_NAMESPACE=

# Cloudflare R2 Configuration (if using R2 for file storage)
CF_R2_ACCESS_KEY_ID=your_r2_access_key
CF_R2_SECRET_ACCESS_KEY=your_r2_secret_key
CF_R2_BUCKET_NAME=your_bucket_name
CF_R2_ENDPOINT=https://your-account-id.r2.cloudflarestorage.com
`;
}

/**
 * Generate README.md file
 */
function generateReadme(pipeline: PipelineData): string {
  return `# MCP Pipeline: ${pipeline.metadata.purpose}

This is an MCP (Model Context Protocol) server generated by Contexto for the pipeline "${pipeline.metadata.purpose}".

## Overview

- **Pipeline ID**: ${pipeline.id}
- **Source File**: ${pipeline.metadata.fileName}
- **Vector Store**: ${pipeline.metadata.vectorStore}
- **Chunks**: ${pipeline.metadata.chunksCount}
- **Chunk Size**: ${pipeline.metadata.chunkSize} characters
- **Overlap**: ${pipeline.metadata.overlap} characters

## Installation

1. Install dependencies:
   \`\`\`bash
   npm install
   \`\`\`

2. Copy \`.env.example\` to \`.env\` and configure your API keys:
   \`\`\`bash
   cp .env.example .env
   \`\`\`

3. Start the server:
   \`\`\`bash
   npm start
   \`\`\`

## Available Tools

### search_knowledge
Search the knowledge base for relevant information.

**Parameters:**
- \`query\` (string, required): The search query
- \`limit\` (number, optional): Maximum number of results (default: ${pipeline.config.retrieval.topK})

### get_pipeline_info
Get information about this pipeline configuration.

## Configuration

This MCP server requires the following environment variables:

- \`AZURE_OPENAI_API_KEY\`: Your Azure OpenAI API key
- \`AZURE_OPENAI_ENDPOINT\`: Your Azure OpenAI endpoint
- \`PINECONE_API_KEY\`: Your Pinecone API key
- \`PINECONE_ENVIRONMENT\`: Your Pinecone environment

## Generated by Contexto

This MCP server was automatically generated by [Contexto](https://contexto.dev) on ${new Date().toISOString()}.
`;
}

/**
 * Create a ZIP archive from a directory
 */
async function createZipArchive(sourceDir: string, outputPath: string): Promise<void> {
  return new Promise((resolve, reject) => {
    const output = fs.createWriteStream(outputPath);
    const archive = archiver('zip', { zlib: { level: 9 } });

    output.on('close', () => {
      console.log(`Archive created: ${archive.pointer()} total bytes`);
      resolve();
    });

    archive.on('error', (err: any) => {
      reject(err);
    });

    archive.pipe(output);
    archive.directory(sourceDir, false);
    archive.finalize();
  });
}

/**
 * Export MCP pipeline bundle to R2 storage
 */
export async function exportMCPBundle(pipelineId: string, userId: string): Promise<{
  exportId: string;
  downloadUrl: string;
  r2Key: string;
}> {
  try {
    // Fetch pipeline data from Firestore
    const db = await getFirestore();
    const pipelineDoc = await db.collection('pipelines').doc(pipelineId).get();
    
    if (!pipelineDoc.exists) {
      throw new Error('Pipeline not found');
    }
    
    const pipelineData = pipelineDoc.data() as PipelineData;
    
    // Create temporary directory for MCP files
    const tempDir = path.join(os.tmpdir(), `mcp-export-${pipelineId}-${Date.now()}`);
    await fs.promises.mkdir(tempDir, { recursive: true });
    
    try {
      // Generate MCP server files
      await fs.promises.writeFile(
        path.join(tempDir, 'package.json'),
        generatePackageJson(pipelineId)
      );
      
      await fs.promises.writeFile(
        path.join(tempDir, 'server.js'),
        generateServerJs(pipelineData)
      );
      
      await fs.promises.writeFile(
        path.join(tempDir, '.env.example'),
        generateEnvExample()
      );
      
      await fs.promises.writeFile(
        path.join(tempDir, 'README.md'),
        generateReadme(pipelineData)
      );

      // Ensure Heroku runs a web dyno
      await fs.promises.writeFile(
        path.join(tempDir, 'Procfile'),
        'web: node server.js\n'
      );
      
      // Create ZIP archive
      // Create the ZIP outside of sourceDir to avoid self-inclusion
      const zipPath = path.join(os.tmpdir(), `mcp-pipeline-${pipelineId}-${Date.now()}.zip`);
      await createZipArchive(tempDir, zipPath);
      
      // Upload to R2
      const exportId = uuidv4();
      const r2Key = `users/${userId}/exports/${exportId}/mcp-pipeline.zip`;
      const uploadUrl = await generateUploadUrl(r2Key, 'application/zip');
      
      // Read ZIP file and upload
      const zipBuffer = await fs.promises.readFile(zipPath);
      const uploadResponse = await fetch(uploadUrl, {
        method: 'PUT',
        body: zipBuffer,
        headers: {
          'Content-Type': 'application/zip',
          'Content-Length': zipBuffer.length.toString()
        }
      });
      
      if (!uploadResponse.ok) {
        throw new Error(`Failed to upload MCP bundle: ${uploadResponse.statusText}`);
      }
      
      // Store export metadata in Firestore
      const exportMetadata = {
        id: exportId,
        pipelineId,
        userId,
        r2Key,
        fileName: 'mcp-pipeline.zip',
        fileSize: zipBuffer.length,
        createdAt: new Date(),
        status: 'completed'
      };
      
      await db.collection('exports').doc(exportId).set(exportMetadata);
      
      // Generate a short-lived private download URL for secure deployment
      const { generateMCPDownloadUrl } = await import('@/lib/r2-client');
      const downloadUrl = await generateMCPDownloadUrl(r2Key);
      
      return {
        exportId,
        downloadUrl,
        r2Key
      };
      
    } finally {
      // Clean up temporary directory
      await fs.promises.rm(tempDir, { recursive: true, force: true });
    }
    
  } catch (error) {
    console.error('MCP export error:', error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to export MCP bundle: ${errorMessage}`);
  }
}
