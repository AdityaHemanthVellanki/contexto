import { generateUploadUrl } from '@/lib/r2-client';
import { getFirestore } from '@/lib/firebase-admin';
import { v4 as uuidv4 } from 'uuid';
import * as fs from 'fs';
import * as path from 'path';
import * as os from 'os';
import archiver from 'archiver';

interface PipelineData {
  id: string;
  userId: string;
  metadata: {
    fileName: string;
    purpose: string;
    vectorStore: string;
    chunksCount: number;
    chunkSize: number;
    overlap: number;
    indexName?: string;
    namespace?: string;
  };
  config: {
    embedding: {
      model: string;
      provider: string;
    };
    indexing: {
      backend: string;
    };
    retrieval: {
      topK: number;
      searchType: string;
    };
  };
}

/**
 * Generate MCP server package.json
 */
function generatePackageJson(pipelineId: string): string {
  return JSON.stringify({
    name: `mcp-pipeline-${pipelineId}`,
    version: '1.0.0',
    description: 'MCP Server generated by Contexto',
    main: 'server.js',
    scripts: {
      start: 'node server.js',
      dev: 'node server.js'
    },
    dependencies: {
      '@modelcontextprotocol/sdk': '^0.4.0',
      'dotenv': '^16.0.0',
      'express': '^4.18.2',
      '@pinecone-database/pinecone': '^4.0.0',
      'openai': '^4.56.0',
      'firebase-admin': '^12.5.0',
      'cors': '^2.8.5'
    },
    engines: {
      node: '>=18.0.0'
    }
  }, null, 2);
}

/**
 * Generate MCP server.js file
 */
function generateServerJs(pipeline: PipelineData): string {
  const md = (pipeline && (pipeline as any).metadata) ? (pipeline as any).metadata : {} as any;
  const cfg = (pipeline && (pipeline as any).config) ? (pipeline as any).config : {} as any;
  const ret = (cfg && cfg.retrieval) ? cfg.retrieval : { topK: 5, searchType: 'similarity' } as any;
  const defaultIndexName = (pipeline as any).indexName || md.indexName || 'ctx-shared';
  const defaultNamespace = (pipeline as any).namespace || md.namespace || '';
  const topK = Number(ret.topK ?? 5) || 5;
  const fileName = md.fileName || 'document.txt';
  const purpose = md.purpose || 'Contexto MCP';
  const vectorStore = md.vectorStore || 'pinecone';
  const chunksCount = typeof md.chunksCount === 'number' ? md.chunksCount : 0;
  const chunkSize = typeof md.chunkSize === 'number' ? md.chunkSize : 500;
  const overlap = typeof md.overlap === 'number' ? md.overlap : 50;
  const searchType = ret.searchType || 'similarity';
  return `#!/usr/bin/env node

const { Server } = require('@modelcontextprotocol/sdk/server/index.js');
const { StdioServerTransport } = require('@modelcontextprotocol/sdk/server/stdio.js');
const { CallToolRequestSchema, ListToolsRequestSchema } = require('@modelcontextprotocol/sdk/types.js');
const express = require('express');
const { Pinecone } = require('@pinecone-database/pinecone');
const OpenAI = require('openai');
const admin = require('firebase-admin');
const cors = require('cors');

// Load environment variables
require('dotenv').config();

// Optional Firestore logging via Firebase Admin
let firestore = null;
try {
  if (process.env.FIREBASE_PROJECT_ID && process.env.FIREBASE_CLIENT_EMAIL && process.env.FIREBASE_PRIVATE_KEY) {
    const privateKey = (process.env.FIREBASE_PRIVATE_KEY || '').replace(/\\n/g, '\n');
    admin.initializeApp({
      credential: admin.credential.cert({
        projectId: process.env.FIREBASE_PROJECT_ID,
        clientEmail: process.env.FIREBASE_CLIENT_EMAIL,
        privateKey
      })
    });
    firestore = admin.firestore();
    console.error('Firebase Admin initialized for MCP server logging');
  }
} catch (e) {
  console.error('Failed to initialize Firebase Admin (logging disabled):', e && e.message ? e.message : e);
}

const server = new Server({
  name: 'contexto-pipeline-${pipeline.id}',
  version: '1.0.0',
  description: 'RAG pipeline for ${purpose}'
}, {
  capabilities: {
    tools: {}
  }
});

// Tool definitions
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: 'search_knowledge',
        description: 'Search the knowledge base for relevant information',
        inputSchema: {
          type: 'object',
          properties: {
            query: {
              type: 'string',
              description: 'The search query'
            },
            limit: {
              type: 'number',
              description: 'Maximum number of results to return',
              default: ${topK}
            }
          },
          required: ['query']
        }
      },
      {
        name: 'get_pipeline_info',
        description: 'Get information about this pipeline',
        inputSchema: {
          type: 'object',
          properties: {}
        }
      }
    ]
  };
});

// Tool handlers
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;
  
  switch (name) {
    case 'search_knowledge':
      return await searchKnowledge(args.query, args.limit || ${topK});
    
    case 'get_pipeline_info':
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              pipelineId: '${pipeline.id}',
              purpose: '${purpose}',
              vectorStore: '${vectorStore}',
              chunksCount: ${chunksCount},
              chunkSize: ${chunkSize},
              overlap: ${overlap},
              retrievalConfig: {
                topK: ${topK},
                searchType: '${searchType}'
              }
            }, null, 2)
          }
        ]
      };
    
    default:
      throw new Error(\`Unknown tool: \${name}\`);
  }
});

// Start a minimal HTTP server to satisfy Heroku's web dyno requirements
const app = express();
app.use(cors());
app.use(express.json());
const PORT = process.env.PORT || 3000;
app.get('/', (_req, res) => res.send('Contexto MCP server is running for pipeline ${pipeline.id}'));
app.get('/health', (_req, res) => res.send('ok'));
app.post('/query', async (req, res) => {
  try {
    const { query, limit } = req.body || {};
    if (!query || typeof query !== 'string') {
      return res.status(400).json({ error: 'Missing query (string)' });
    }
    const result = await searchKnowledge(query, limit);
    res.json(result);
  } catch (e) {
    res.status(500).json({ error: e && e.message ? e.message : 'Query error' });
  }
});
app.listen(PORT, () => {
  console.error('HTTP server listening on port ' + PORT);
});

// Search function implementation
async function searchKnowledge(query, limit = ${topK}) {
  try {
    // Validate env
    const AZKEY = process.env.AZURE_OPENAI_API_KEY;
    const AZEP = (process.env.AZURE_OPENAI_ENDPOINT || '').replace(/\/$/, '')
    const AZ_EMBED_DEP = process.env.AZURE_OPENAI_DEPLOYMENT_EMBEDDING;
    const AZ_CHAT_DEP = process.env.AZURE_OPENAI_DEPLOYMENT_GPT4 || process.env.AZURE_OPENAI_DEPLOYMENT_TURBO;
    const AZ_API_VERSION = process.env.AZURE_OPENAI_API_VERSION || '2024-02-15-preview';
    const PINECONE_API_KEY = process.env.PINECONE_API_KEY;
    const DEFAULT_INDEX_NAME = process.env.PINECONE_INDEX || process.env.PINECONE_INDEX_NAME || '${defaultIndexName}';
    const DEFAULT_NAMESPACE = process.env.PINECONE_NAMESPACE || '${defaultNamespace}';

    if (!PINECONE_API_KEY) {
      throw new Error('Missing PINECONE_API_KEY');
    }
    if (!AZKEY || !AZEP || !AZ_EMBED_DEP) {
      throw new Error('Missing Azure OpenAI embedding configuration (AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_EMBEDDING)');
    }

    // Init Pinecone client
    const pc = new Pinecone({ apiKey: PINECONE_API_KEY });
    const index = pc.index(DEFAULT_INDEX_NAME);

    // Init Azure OpenAI clients
    const embedClient = new OpenAI({
      apiKey: AZKEY,
      baseURL: AZEP + '/openai/deployments/' + AZ_EMBED_DEP,
      defaultQuery: { 'api-version': AZ_API_VERSION },
      defaultHeaders: { 'api-key': AZKEY }
    });

    const chatEnabled = Boolean(AZ_CHAT_DEP);
    const chatClient = chatEnabled ? new OpenAI({
      apiKey: AZKEY,
      baseURL: AZEP + '/openai/deployments/' + AZ_CHAT_DEP,
      defaultQuery: { 'api-version': AZ_API_VERSION },
      defaultHeaders: { 'api-key': AZKEY }
    }) : null;

    // Create query embedding
    const emb = await embedClient.embeddings.create({ input: query, model: AZ_EMBED_DEP });
    const vector = emb?.data?.[0]?.embedding;
    if (!vector) throw new Error('Failed to generate query embedding');

    // Query Pinecone (namespace per user)
    const ns = DEFAULT_NAMESPACE || undefined;
    const results = await index.namespace(ns).query({
      topK: Number(limit) || ${topK},
      vector,
      includeMetadata: true
    });

    const matches = results?.matches || [];
    if (!matches.length) {
      return {
        content: [{ type: 'text', text: \`No results found for: "\${query}"\` }]
      };
    }

    // Build context from top matches
    const sources = matches.map((m, i) => ({
      score: m.score,
      text: m?.metadata?.text || '',
      fileName: m?.metadata?.fileName || '${fileName}',
      chunkIndex: m?.metadata?.chunkIndex ?? i
    }));
    const context = sources.map((s, i) => \`[\${i+1}] \${s.text}\`).join('\n\n');

    // Optional summarization via Azure OpenAI
    let answer = \`Top \${Math.min(sources.length, Number(limit) || ${topK})} results for: "\${query}"\n\n\${sources.map((s,i)=>\`[\${i+1}] (\${(s.score||0).toFixed(4)}) \${s.text.substring(0,300)}\${s.text.length>300?'â€¦':''}\`).join('\n\n')}\`;
    if (chatEnabled && chatClient) {
      try {
        const completion = await chatClient.chat.completions.create({
          model: AZ_CHAT_DEP,
          temperature: 0.2,
          messages: [
            { role: 'system', content: 'You are a helpful assistant. Use the provided context excerpts to answer.' },
            { role: 'user', content: \`Context:\n\${context}\n\nQuestion: \${query}\n\nProvide a concise answer and cite sources as [1], [2], etc.\` }
          ]
        });
        const msg = completion?.choices?.[0]?.message?.content;
        if (msg) answer = msg;
      } catch (e) {
        // Fallback to raw snippets if chat fails
      }
    }

    // Return MCP tool response format
    const response = {
      content: [
        { type: 'text', text: answer },
        { type: 'text', text: \`\n\nSources:\n\${sources.map((s,i)=>\`[\${i+1}] \${s.fileName}#\${s.chunkIndex}\`).join('\n')}\` }
      ]
    };

    // Optional Firestore query logging
    try {
      if (firestore) {
        await firestore.collection('mcp_queries').add({
          pipelineId: '${pipeline.id}',
          userId: process.env.MCP_USER_ID || null,
          query,
          answer: typeof answer === 'string' ? answer.slice(0, 4000) : '',
          topK: Number(limit) || ${topK},
          ts: new Date(),
          sources: sources.slice(0, ${topK})
        });
      }
    } catch (logErr) {
      console.error('Failed to log MCP query:', logErr && logErr.message ? logErr.message : logErr);
    }

    return response;
  } catch (error) {
    return {
      content: [
        {
          type: 'text',
          text: \`Error searching knowledge base: \${error.message}\`
        }
      ]
    };
  }
}

// Start the server
async function main() {
  const transport = new StdioServerTransport();
  await server.connect(transport);
  console.error('MCP Server started for pipeline ${pipeline.id}');
}

if (require.main === module) {
  main().catch(console.error);
}
`;
}

/**
 * Generate .env.example file
 */
function generateEnvExample(): string {
  return `# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_EMBEDDING=text-embedding-ada-002
AZURE_OPENAI_DEPLOYMENT_TURBO=gpt-35-turbo
AZURE_OPENAI_DEPLOYMENT_GPT4=gpt-4
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX_NAME=contexto-shared
# or
PINECONE_INDEX=contexto-shared
PINECONE_NAMESPACE=

# Cloudflare R2 Configuration (if using R2 for file storage)
CF_R2_ACCESS_KEY_ID=your_r2_access_key
CF_R2_SECRET_ACCESS_KEY=your_r2_secret_key
CF_R2_BUCKET_NAME=your_bucket_name
CF_R2_ENDPOINT=https://your-account-id.r2.cloudflarestorage.com
`;
}

/**
 * Generate README.md file
 */
function generateReadme(pipeline: PipelineData): string {
  const md = (pipeline && (pipeline as any).metadata) ? (pipeline as any).metadata : {} as any;
  const cfg = (pipeline && (pipeline as any).config) ? (pipeline as any).config : {} as any;
  const ret = (cfg && cfg.retrieval) ? cfg.retrieval : {} as any;
  const purpose = md.purpose || 'Contexto MCP';
  const fileName = md.fileName || 'document.txt';
  const vectorStore = md.vectorStore || 'pinecone';
  const chunksCount = typeof md.chunksCount === 'number' ? md.chunksCount : 0;
  const chunkSize = typeof md.chunkSize === 'number' ? md.chunkSize : 500;
  const overlap = typeof md.overlap === 'number' ? md.overlap : 50;
  const topK = Number(ret.topK ?? 5) || 5;

  return `# MCP Pipeline: ${purpose}

This is an MCP (Model Context Protocol) server generated by Contexto for the pipeline "${purpose}".

## Overview

- **Pipeline ID**: ${pipeline.id}
- **Source File**: ${fileName}
- **Vector Store**: ${vectorStore}
- **Chunks**: ${chunksCount}
- **Chunk Size**: ${chunkSize} characters
- **Overlap**: ${overlap} characters

## Installation

1. Install dependencies:
   \`\`\`bash
   npm install
   \`\`\`

2. Copy \`.env.example\` to \`.env\` and configure your API keys:
   \`\`\`bash
   cp .env.example .env
   \`\`\`

3. Start the server:
   \`\`\`bash
   npm start
   \`\`\`

## Available Tools

### search_knowledge
Search the knowledge base for relevant information.

**Parameters:**
- \`query\` (string, required): The search query
- \`limit\` (number, optional): Maximum number of results (default: ${topK})

### get_pipeline_info
Get information about this pipeline configuration.

## Configuration

This MCP server requires the following environment variables:

- \`AZURE_OPENAI_API_KEY\`: Your Azure OpenAI API key
- \`AZURE_OPENAI_ENDPOINT\`: Your Azure OpenAI endpoint
- \`PINECONE_API_KEY\`: Your Pinecone API key
- \`PINECONE_ENVIRONMENT\`: Your Pinecone environment

## Generated by Contexto

This MCP server was automatically generated by [Contexto](https://contexto.dev) on ${new Date().toISOString()}.
`;
}

/**
 * Create a TAR.GZ archive from a directory (Heroku builds require tarball)
 */
async function createTarGzArchive(sourceDir: string, outputPath: string): Promise<void> {
  return new Promise((resolve, reject) => {
    const output = fs.createWriteStream(outputPath);
    const archive = archiver('tar', { gzip: true, gzipOptions: { level: 9 } });

    output.on('close', () => {
      console.log(`Archive created: ${archive.pointer()} total bytes`);
      resolve();
    });

    archive.on('error', (err: any) => {
      reject(err);
    });

    archive.pipe(output);
    archive.directory(sourceDir, false);
    archive.finalize();
  });
}

/**
 * Export MCP pipeline bundle to R2 storage
 */
export async function exportMCPBundle(pipelineId: string, userId: string): Promise<{
  exportId: string;
  downloadUrl: string;
  r2Key: string;
}> {
  try {
    // Fetch pipeline data from Firestore
    const db = await getFirestore();
    const pipelineDoc = await db.collection('pipelines').doc(pipelineId).get();
    
    if (!pipelineDoc.exists) {
      throw new Error('Pipeline not found');
    }
    
    const pipelineRaw = pipelineDoc.data() as Partial<PipelineData> | undefined;
    const pipelineData: PipelineData = {
      id: pipelineId,
      userId: (pipelineRaw as any)?.userId || userId,
      metadata: (pipelineRaw as any)?.metadata || ({} as any),
      config: (pipelineRaw as any)?.config || ({} as any)
    } as any;
    
    // Create temporary directory for MCP files
    const tempDir = path.join(os.tmpdir(), `mcp-export-${pipelineId}-${Date.now()}`);
    await fs.promises.mkdir(tempDir, { recursive: true });
    
    try {
      // Generate MCP server files
      await fs.promises.writeFile(
        path.join(tempDir, 'package.json'),
        generatePackageJson(pipelineId)
      );
      
      await fs.promises.writeFile(
        path.join(tempDir, 'server.js'),
        generateServerJs(pipelineData)
      );
      
      await fs.promises.writeFile(
        path.join(tempDir, '.env.example'),
        generateEnvExample()
      );
      
      await fs.promises.writeFile(
        path.join(tempDir, 'README.md'),
        generateReadme(pipelineData)
      );

      // Ensure Heroku runs a web dyno
      await fs.promises.writeFile(
        path.join(tempDir, 'Procfile'),
        'web: node server.js\n'
      );
      
      // Create TAR.GZ archive outside of sourceDir to avoid self-inclusion
      const tarPath = path.join(os.tmpdir(), `mcp-pipeline-${pipelineId}-${Date.now()}.tar.gz`);
      await createTarGzArchive(tempDir, tarPath);
      
      // Upload to R2
      const exportId = uuidv4();
      const r2Key = `users/${userId}/exports/${exportId}/mcp-pipeline.tar.gz`;
      const uploadUrl = await generateUploadUrl(r2Key, 'application/gzip');
      
      // Read TAR.GZ file and upload
      const tarBuffer = await fs.promises.readFile(tarPath);
      const uploadResponse = await fetch(uploadUrl, {
        method: 'PUT',
        body: tarBuffer,
        headers: {
          'Content-Type': 'application/gzip',
          'Content-Length': tarBuffer.length.toString()
        }
      });
      
      if (!uploadResponse.ok) {
        throw new Error(`Failed to upload MCP bundle: ${uploadResponse.statusText}`);
      }
      
      // Store export metadata in Firestore
      const exportMetadata = {
        id: exportId,
        pipelineId,
        userId,
        r2Key,
        fileName: 'mcp-pipeline.tar.gz',
        fileSize: tarBuffer.length,
        createdAt: new Date(),
        status: 'completed'
      };
      
      await db.collection('exports').doc(exportId).set(exportMetadata);
      
      // Generate a short-lived private download URL for secure deployment
      const { generateMCPDownloadUrl } = await import('@/lib/r2-client');
      const downloadUrl = await generateMCPDownloadUrl(r2Key);
      
      return {
        exportId,
        downloadUrl,
        r2Key
      };
      
    } finally {
      // Clean up temporary directory
      await fs.promises.rm(tempDir, { recursive: true, force: true });
    }
    
  } catch (error) {
    console.error('MCP export error:', error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    throw new Error(`Failed to export MCP bundle: ${errorMessage}`);
  }
}
